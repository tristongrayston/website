---
title: "Introduction to Variational Auto Encoders"
date: 2023-01-01
---

This project marked my true entry point into machine learning, serving as a foundational stepping stone for more advanced, research-oriented work. I undertook this challenge in my second year to prepare for a complex project involving astronomy data, deciding that a hands-on implementation of a Variational Autoencoder (VAE) would be the best way to build a robust understanding of neural networks from the ground up.

The journey was a practical and intensive lesson. I grappled with core challenges inherent in training VAEs, most notably the issue of **posterior collapse**, where the decoder learns to ignore the latent code, leading to generic outputs. Overcoming this forced me to dive deep into the model's theoretical underpinnings, experiment with different network architectures, and fine-tune the loss function. This struggle was instrumental in cementing my understanding of how generative models function and learn.

The project culminated in a presentation to the **UVicAI club**, where I had the opportunity to share my findings and demystify the inner workings of Variational Autoencoders for my peers. This experience not only solidified my technical knowledge but also honed my ability to communicate complex topics clearly and effectively. 